{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud_fJ1-lXJz7",
        "outputId": "8c528c1a-c4dd-4c7e-d4a1-da069872c433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def extract_10_frames(video_path, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, 10, dtype=int)\n",
        "\n",
        "    count = 0\n",
        "    saved = 0\n",
        "    for idx in range(total_frames):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if idx in frame_indices:\n",
        "            frame_name = f\"frame_{saved:02d}.jpg\"\n",
        "            frame_path = os.path.join(output_dir, frame_name)\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved += 1\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"✅ Saved {saved} frames to: {output_dir}\")\n",
        "\n",
        "# ✅ Use raw strings to avoid \\ errors\n",
        "video_path = r\"/content/drive/MyDrive/Research/FT3-LA.mp4\"\n",
        "output_dir = r\"/content/drive/MyDrive/Research/Combined Data/Frames\"\n",
        "\n",
        "extract_10_frames(video_path, output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68e94iwUXfD_",
        "outputId": "fcdd2cf0-9b1a-4478-fe17-dcd00ffdff81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 10 frames to: /content/drive/MyDrive/Research/Combined Data/Frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FKViaBwdU75",
        "outputId": "d9aa1b9c-6a76-4d46-c7f9-b5fe01b5f412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.167-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.167-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.167 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def annotate_and_save_labels(frames_folder, output_folder, label_folder, yolo_model_path='yolov8n.pt', conf_thresh=0.3):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    os.makedirs(label_folder, exist_ok=True)\n",
        "\n",
        "    model = YOLO(yolo_model_path)\n",
        "    frame_files = sorted([f for f in os.listdir(frames_folder) if f.endswith('.jpg')])\n",
        "\n",
        "    for file in frame_files:\n",
        "        img_path = os.path.join(frames_folder, file)\n",
        "        img = cv2.imread(img_path)\n",
        "        height, width = img.shape[:2]\n",
        "\n",
        "        # Run YOLOv8 inference\n",
        "        results = model(img)[0]\n",
        "\n",
        "        best_box = None\n",
        "        best_conf = 0.0\n",
        "\n",
        "        # Find the most confident 'person' (class 0)\n",
        "        for box in results.boxes:\n",
        "            cls_id = int(box.cls)\n",
        "            conf = float(box.conf)\n",
        "            if cls_id == 0 and conf > conf_thresh and conf > best_conf:\n",
        "                best_box = box\n",
        "                best_conf = conf\n",
        "\n",
        "        label_lines = []\n",
        "\n",
        "        if best_box:\n",
        "            x1, y1, x2, y2 = map(float, best_box.xyxy[0])\n",
        "\n",
        "            # Convert to YOLO format (normalized)\n",
        "            x_center = (x1 + x2) / 2 / width\n",
        "            y_center = (y1 + y2) / 2 / height\n",
        "            box_width = (x2 - x1) / width\n",
        "            box_height = (y2 - y1) / height\n",
        "\n",
        "            label_lines.append(f\"0 {x_center:.6f} {y_center:.6f} {box_width:.6f} {box_height:.6f}\")\n",
        "\n",
        "            # Draw the bounding box and label\n",
        "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "            cv2.putText(img, f\"Athlete {best_conf:.2f}\", (int(x1), int(y1) - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Save annotated image\n",
        "        annotated_img_path = os.path.join(output_folder, file)\n",
        "        cv2.imwrite(annotated_img_path, img)\n",
        "\n",
        "        # Save label file (same name but .txt)\n",
        "        label_filename = os.path.splitext(file)[0] + \".txt\"\n",
        "        label_path = os.path.join(label_folder, label_filename)\n",
        "        with open(label_path, 'w') as f:\n",
        "            f.write('\\n'.join(label_lines))\n",
        "\n",
        "    print(f\"✅ Annotated only the athlete in {len(frame_files)} frames.\")\n",
        "    print(f\"🖼️ Annotated images → {output_folder}\")\n",
        "    print(f\"📝 YOLO .txt labels → {label_folder}\")\n",
        "\n",
        "# Example usage:\n",
        "frames_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames\"\n",
        "output_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameAnnotated\"\n",
        "label_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameLabels\"\n",
        "yolo_model_path = \"yolov8n.pt\"  # or your custom model path\n",
        "\n",
        "annotate_and_save_labels(frames_folder, output_folder, label_folder, yolo_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "827EwWpZXuze",
        "outputId": "8045f6e6-da98-4e32-e257-068703724c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 6 persons, 132.0ms\n",
            "Speed: 2.8ms preprocess, 132.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 143.0ms\n",
            "Speed: 4.0ms preprocess, 143.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 133.7ms\n",
            "Speed: 3.5ms preprocess, 133.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 129.8ms\n",
            "Speed: 3.8ms preprocess, 129.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 134.8ms\n",
            "Speed: 4.1ms preprocess, 134.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 1 sports ball, 1 tennis racket, 128.0ms\n",
            "Speed: 3.0ms preprocess, 128.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 130.1ms\n",
            "Speed: 3.7ms preprocess, 130.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 145.1ms\n",
            "Speed: 3.3ms preprocess, 145.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 persons, 1 backpack, 128.2ms\n",
            "Speed: 3.7ms preprocess, 128.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 13 persons, 132.8ms\n",
            "Speed: 3.7ms preprocess, 132.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Annotated only the athlete in 10 frames.\n",
            "🖼️ Annotated images → /content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameAnnotated\n",
            "📝 YOLO .txt labels → /content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameLabels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def annotate_and_save_labels(frames_folder, output_folder, label_folder, yolo_model_path='yolov8n.pt', conf_thresh=0.25):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    os.makedirs(label_folder, exist_ok=True)\n",
        "\n",
        "    # Load YOLOv8 model (pretrained or custom)\n",
        "    model = YOLO(yolo_model_path)\n",
        "\n",
        "    # Collect all image files\n",
        "    frame_files = sorted([f for f in os.listdir(frames_folder) if f.endswith('.jpg') or f.endswith('.png')])\n",
        "\n",
        "    total_labels = 0\n",
        "\n",
        "    for file in frame_files:\n",
        "        img_path = os.path.join(frames_folder, file)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"⚠️ Could not read image: {file}\")\n",
        "            continue\n",
        "\n",
        "        height, width = img.shape[:2]\n",
        "        results = model(img)[0]  # Run inference\n",
        "\n",
        "        best_box = None\n",
        "        best_conf = 0.0\n",
        "\n",
        "        # Loop through detections\n",
        "        for box in results.boxes:\n",
        "            cls_id = int(box.cls)\n",
        "            conf = float(box.conf)\n",
        "\n",
        "            if cls_id == 0 and conf > conf_thresh and conf > best_conf:\n",
        "                best_box = box\n",
        "                best_conf = conf\n",
        "\n",
        "        label_lines = []\n",
        "\n",
        "        if best_box:\n",
        "            x1, y1, x2, y2 = map(float, best_box.xyxy[0].tolist())\n",
        "\n",
        "            # Convert to YOLO format\n",
        "            x_center = (x1 + x2) / 2 / width\n",
        "            y_center = (y1 + y2) / 2 / height\n",
        "            box_width = (x2 - x1) / width\n",
        "            box_height = (y2 - y1) / height\n",
        "\n",
        "            label_lines.append(f\"0 {x_center:.6f} {y_center:.6f} {box_width:.6f} {box_height:.6f}\")\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "            cv2.putText(img, f\"Athlete {best_conf:.2f}\", (int(x1), int(y1) - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Save annotated image\n",
        "        annotated_img_path = os.path.join(output_folder, file)\n",
        "        cv2.imwrite(annotated_img_path, img)\n",
        "\n",
        "        # Save label file only if person was found\n",
        "        if label_lines:\n",
        "            label_filename = os.path.splitext(file)[0] + \".txt\"\n",
        "            label_path = os.path.join(label_folder, label_filename)\n",
        "            with open(label_path, 'w') as f:\n",
        "                f.write('\\n'.join(label_lines))\n",
        "            total_labels += 1\n",
        "            print(f\"✅ {file}: Athlete detected (conf={best_conf:.2f}), label saved.\")\n",
        "        else:\n",
        "            print(f\"❌ {file}: No person detected above threshold.\")\n",
        "\n",
        "    print(f\"\\n✅ Done: {total_labels} athlete labels created.\")\n",
        "    print(f\"🖼️ Annotated images → {output_folder}\")\n",
        "    print(f\"📝 YOLO .txt labels → {label_folder}\")\n",
        "\n",
        "# --- Example Usage ---\n",
        "frames_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames\"\n",
        "output_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameAnnotated\"\n",
        "label_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameLabels\"\n",
        "yolo_model_path = \"yolov8n.pt\"  # or your custom-trained .pt\n",
        "\n",
        "annotate_and_save_labels(frames_folder, output_folder, label_folder, yolo_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIuKU_GMdK8V",
        "outputId": "8ea2fe38-aea7-49ec-ec15-73314f7f5ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 6 persons, 142.5ms\n",
            "Speed: 5.1ms preprocess, 142.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_00.jpg: Athlete detected (conf=0.77), label saved.\n",
            "\n",
            "0: 384x640 4 persons, 128.3ms\n",
            "Speed: 3.7ms preprocess, 128.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_01.jpg: Athlete detected (conf=0.81), label saved.\n",
            "\n",
            "0: 384x640 5 persons, 140.1ms\n",
            "Speed: 3.6ms preprocess, 140.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_02.jpg: Athlete detected (conf=0.83), label saved.\n",
            "\n",
            "0: 384x640 4 persons, 141.6ms\n",
            "Speed: 3.8ms preprocess, 141.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_03.jpg: Athlete detected (conf=0.87), label saved.\n",
            "\n",
            "0: 384x640 5 persons, 143.6ms\n",
            "Speed: 3.1ms preprocess, 143.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_04.jpg: Athlete detected (conf=0.91), label saved.\n",
            "\n",
            "0: 384x640 6 persons, 1 sports ball, 1 tennis racket, 127.0ms\n",
            "Speed: 3.6ms preprocess, 127.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_05.jpg: Athlete detected (conf=0.92), label saved.\n",
            "\n",
            "0: 384x640 9 persons, 147.3ms\n",
            "Speed: 3.8ms preprocess, 147.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_06.jpg: Athlete detected (conf=0.77), label saved.\n",
            "\n",
            "0: 384x640 12 persons, 133.5ms\n",
            "Speed: 3.6ms preprocess, 133.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_07.jpg: Athlete detected (conf=0.85), label saved.\n",
            "\n",
            "0: 384x640 15 persons, 1 backpack, 169.7ms\n",
            "Speed: 3.5ms preprocess, 169.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_08.jpg: Athlete detected (conf=0.91), label saved.\n",
            "\n",
            "0: 384x640 13 persons, 132.3ms\n",
            "Speed: 3.7ms preprocess, 132.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ frame_09.jpg: Athlete detected (conf=0.92), label saved.\n",
            "\n",
            "✅ Done: 10 athlete labels created.\n",
            "🖼️ Annotated images → /content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameAnnotated\n",
            "📝 YOLO .txt labels → /content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameLabels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mediapipe opencv-python numpy\n"
      ],
      "metadata": {
        "id": "wWQF7uL1ej7s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c9cece0-3a9d-4888-9fa2-35e7c7ce780b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, numpy, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "33776a71108c4be6aa65ad06e0863ae0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "# --- Utility: Calculate joint angle ---\n",
        "def calculate_angle(a, b, c):\n",
        "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
        "    ba = a - b\n",
        "    bc = c - b\n",
        "    cos_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
        "    angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
        "    return np.degrees(angle)\n",
        "\n",
        "# --- Extract features from one frame using YOLO label ---\n",
        "def extract_features_from_frame(image, label_path, width, height):\n",
        "    if not os.path.exists(label_path):\n",
        "        return None\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        line = f.readline().strip()\n",
        "        if not line:\n",
        "            return None\n",
        "        _, x_c, y_c, w, h = map(float, line.split())\n",
        "\n",
        "    x1 = int((x_c - w / 2) * width)\n",
        "    y1 = int((y_c - h / 2) * height)\n",
        "    x2 = int((x_c + w / 2) * width)\n",
        "    y2 = int((y_c + h / 2) * height)\n",
        "    x1, y1 = max(x1, 0), max(y1, 0)\n",
        "    x2, y2 = min(x2, width), min(y2, height)\n",
        "\n",
        "    cropped = image[y1:y2, x1:x2]\n",
        "\n",
        "    # Apply MediaPipe Pose\n",
        "    with mp_pose.Pose(static_image_mode=True, model_complexity=1) as pose:\n",
        "        results = pose.process(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
        "        if not results.pose_landmarks:\n",
        "            return None\n",
        "\n",
        "        features = []\n",
        "        for lm in results.pose_landmarks.landmark:\n",
        "            abs_x = (lm.x * (x2 - x1) + x1) / width\n",
        "            abs_y = (lm.y * (y2 - y1) + y1) / height\n",
        "            abs_z = lm.z\n",
        "            vis = lm.visibility\n",
        "            features.extend([abs_x, abs_y, abs_z, vis])  # 33 × 4 = 132\n",
        "\n",
        "        def coord(idx):\n",
        "            return [\n",
        "                results.pose_landmarks.landmark[idx].x * (x2 - x1) + x1,\n",
        "                results.pose_landmarks.landmark[idx].y * (y2 - y1) + y1,\n",
        "            ]\n",
        "\n",
        "        try:\n",
        "            angles = [\n",
        "                calculate_angle(coord(11), coord(13), coord(15)),  # left elbow\n",
        "                calculate_angle(coord(12), coord(14), coord(16)),  # right elbow\n",
        "                calculate_angle(coord(23), coord(25), coord(27)),  # left knee\n",
        "                calculate_angle(coord(24), coord(26), coord(28)),  # right knee\n",
        "                calculate_angle(coord(11), coord(12), coord(14)),  # left shoulder\n",
        "                calculate_angle(coord(12), coord(11), coord(13)),  # right shoulder\n",
        "            ]\n",
        "        except:\n",
        "            angles = [0] * 6\n",
        "\n",
        "        features.extend(angles)  # Total = 138\n",
        "        return features\n",
        "\n",
        "# --- Process all frames in groups of 10 (with padding if needed) ---\n",
        "def process_frames_for_prediction(frame_folder, label_folder, save_path_x):\n",
        "    frame_files = sorted([f for f in os.listdir(frame_folder) if f.endswith('.jpg') or f.endswith('.png')])\n",
        "    total_frames = len(frame_files)\n",
        "    sample_features = []\n",
        "\n",
        "    for idx, frame_file in enumerate(frame_files):\n",
        "        image_path = os.path.join(frame_folder, frame_file)\n",
        "        label_path = os.path.join(label_folder, os.path.splitext(frame_file)[0] + \".txt\")\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"⚠️ Cannot read {frame_file}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        features = extract_features_from_frame(image, label_path, w, h)\n",
        "        if features is None:\n",
        "            print(f\"⚠️ No pose for {frame_file}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        sample_features.append(features)\n",
        "\n",
        "    if len(sample_features) == 0:\n",
        "        print(\"❌ No valid frames for prediction.\")\n",
        "        return\n",
        "\n",
        "    # Pad to 10 frames if needed\n",
        "    while len(sample_features) < 10:\n",
        "        sample_features.append(sample_features[-1])  # repeat last frame\n",
        "\n",
        "    # Trim if more than 10 (optional)\n",
        "    sample_features = sample_features[:10]\n",
        "\n",
        "    data = np.array(sample_features)  # shape: (10, 138)\n",
        "    velocities = np.gradient(data[:, :99], axis=0)\n",
        "    enhanced = np.concatenate([data, velocities], axis=1)  # (10, 237)\n",
        "\n",
        "    X_pred = np.expand_dims(enhanced, axis=0)  # shape: (1, 10, 237)\n",
        "    np.save(save_path_x, X_pred)\n",
        "\n",
        "    print(f\"\\n✅ Saved 1 padded prediction sample.\")\n",
        "    print(f\"Shape: {X_pred.shape}\")\n",
        "    print(f\"📁 File saved to: {save_path_x}\")\n",
        "\n",
        "# --- CONFIG ---\n",
        "frame_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameAnnotated\"\n",
        "label_folder = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/FrameLabels\"\n",
        "save_path_x = r\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\"\n",
        "\n",
        "# --- RUN ---\n",
        "process_frames_for_prediction(frame_folder, label_folder, save_path_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cVP1ke6oSuB",
        "outputId": "63182893-8776-4b6a-e183-0013f440baf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Saved 1 padded prediction sample.\n",
            "Shape: (1, 10, 237)\n",
            "📁 File saved to: /content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install joblib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEA3k5YrpD-s",
        "outputId": "55db6378-46bc-4722-afc6-0107977cc97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "scaler = joblib.load(\"/content/drive/MyDrive/Research/New Updated Model/Model/Improved_scaler.save\")\n",
        "X_pred_raw = np.load(\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\")  # shape (1, 10, 237)\n",
        "\n",
        "# Flatten, scale, reshape\n",
        "X_pred_scaled = scaler.transform(X_pred_raw.reshape(-1, X_pred_raw.shape[-1]))\n",
        "X_pred_scaled = X_pred_scaled.reshape(X_pred_raw.shape)\n",
        "\n",
        "# Then predict\n",
        "prediction = model.predict(X_pred_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLOj4ALS29DA",
        "outputId": "3efb1b59-108b-4667-ab59-421907fce3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/Research/New Updated Model/Model/Improved_best_model.keras\", safe_mode=False)\n",
        "\n",
        "# Load scaler\n",
        "scaler = joblib.load(\"/content/drive/MyDrive/Research/New Updated Model/Model/Improved_scaler.save\")\n",
        "\n",
        "# Load input\n",
        "X_pred = np.load(\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\")\n",
        "\n",
        "# Apply scaler frame-wise\n",
        "original_shape = X_pred.shape  # (1, 10, 237)\n",
        "X_reshaped = X_pred.reshape(-1, original_shape[-1])  # (10, 237)\n",
        "X_scaled = scaler.transform(X_reshaped)\n",
        "X_scaled = X_scaled.reshape(original_shape)  # (1, 10, 237)\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(X_scaled)\n",
        "pred_class = np.argmax(pred[0])\n",
        "confidence = np.max(pred[0])\n",
        "\n",
        "class_map = {\n",
        "    0: \"Good Technique\",\n",
        "    1: \"Low Arm\",\n",
        "    2: \"Poor Left Leg Block\",\n",
        "    3: \"Both Errors\"\n",
        "}\n",
        "\n",
        "print(f\"\\n🎯 Predicted Class: {pred_class} ({class_map[pred_class]})\")\n",
        "print(f\"📊 Confidence: {confidence:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "FXnNGFSwqLxa",
        "outputId": "cc7483ac-b3aa-40ca-9173-11c2125a968e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 6.25000029685907e-05, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': 'sparse_categorical_crossentropy', 'loss_weights': None, 'metrics': ['accuracy'], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}.\n\nException encountered: Could not locate class 'Attention'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Attention', 'config': {'name': 'attention_4', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 132972519694928}}, 'registered_name': 'Attention', 'build_config': {'input_shape': [None, 5, 128]}, 'name': 'attention_4', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5, 128], 'dtype': 'float32', 'keras_history': ['dropout_11', 0, 0]}}], 'kwargs': {}}]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     cls = _retrieve_class_or_fn(\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;34mf\"Could not locate {obj_type} '{name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'Attention'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Attention', 'config': {'name': 'attention_4', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 132972519694928}}, 'registered_name': 'Attention', 'build_config': {'input_shape': [None, 5, 128]}, 'name': 'attention_4', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5, 128], 'dtype': 'float32', 'keras_history': ['dropout_11', 0, 0]}}], 'kwargs': {}}]}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-1192525522.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Research/New Updated Model/Model/Improved_best_model.keras\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 6.25000029685907e-05, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': 'sparse_categorical_crossentropy', 'loss_weights': None, 'metrics': ['accuracy'], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}.\n\nException encountered: Could not locate class 'Attention'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Attention', 'config': {'name': 'attention_4', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 132972519694928}}, 'registered_name': 'Attention', 'build_config': {'input_shape': [None, 5, 128]}, 'name': 'attention_4', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 5, 128], 'dtype': 'float32', 'keras_history': ['dropout_11', 0, 0]}}], 'kwargs': {}}]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load model and input\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/Research/New Updated Model/Model/New_best_model.keras\")\n",
        "X_pred = np.load(\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\")\n",
        "\n",
        "# Print raw model output (before argmax)\n",
        "pred = model.predict(X_pred)\n",
        "print(\"🔢 Softmax Output:\", pred[0])\n",
        "print(\"🎯 Predicted class:\", np.argmax(pred[0]))\n"
      ],
      "metadata": {
        "id": "78z4j-N6qZvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ba6097-1145-4414-cb64-75b73fb165e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a4bfadc7880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "🔢 Softmax Output: [ 0.00015074   0.0026037    0.029255     0.96799]\n",
            "🎯 Predicted class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_pred = np.load(\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\")  # shape should be (N, 10, 237)\n",
        "\n",
        "for i in range(len(X_pred)):\n",
        "    pred = model.predict(X_pred[i:i+1])\n",
        "    print(f\"Sample {i+1}: {pred[0]} → Predicted: {np.argmax(pred[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMDaxss74PlF",
        "outputId": "ef88b16a-8464-4aa2-8518-6efd75db2a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Sample 1: [ 0.00015074   0.0026037    0.029255     0.96799] → Predicted: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_pred = np.load(\"/content/drive/MyDrive/Research/Combined Data/Frames/YOLO/X_pred.npy\")\n",
        "print(\"Shape of prediction input:\", X_pred.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrW-BWUw40kR",
        "outputId": "0a75e1c4-f2d9-487f-80ec-94440eb8b4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prediction input: (1, 10, 237)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(X_pred.shape[0]):\n",
        "    pred = model.predict(X_pred[i:i+1])\n",
        "    print(f\"Sample {i+1}: {pred[0]} → Predicted class: {np.argmax(pred[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVUJeIPr5AFR",
        "outputId": "53e0635f-2f1e-4e70-cbcf-333f5ba6925a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Sample 1: [ 0.00015074   0.0026037    0.029255     0.96799] → Predicted class: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = [...]  # true test labels\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred_labels, target_names=[\"Good\", \"Low Arm\", \"Poor Leg\", \"Both Errors\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "35Ryldbc5Hw0",
        "outputId": "a419a063-2e6f-4564-de65-6578f0edd574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-427649229.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# true test labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Du38JQhU5uLk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}